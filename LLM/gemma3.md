# 무료 Gemini 2.5 API에서 Gemma 3로의 강제 이주기

## Intro

텍스트로부터 시간 정보를 추출하고 정규화하는 용도로 Gemini API를 사용해왔다. 그런데 `gemini-2.0-flash`를 무료로 넉넉하게 쓰던 게 어느 날 갑자기 막혔다. 찾아보니 2025년 12월 초에 Google이 무료 티어를 대폭 축소한 모양이다. 확인해보니 다수의 429 로그가 찍혀있었다.

![1](https://raw.githubusercontent.com/ShanePark/mdblog/main/LLM/gemma3.assets/1.webp)

그래서 부랴부랴 확인해보니 `gemini-2.5-flash-lite`가 그나마 무료로 사용 가능하지만 RPM 10, RPD 20이라는 수치는 도저히 쓸 수 없는 수준이었다. 

![3](https://raw.githubusercontent.com/ShanePark/mdblog/main/LLM/gemma3.assets/3.webp)

> gemini-2.5-flash-lite 사용 시 곧바로 limit에 다다른다.

결국 Gemma 3로 갈아타게 되었고, 프롬프트 엔지니어링을 거치니 어느 정도 기존과 동일하게 동작하게 되어 이 후기 글을 작성한다.

프론티어 LLM의 성능이 거침없이 올라가는 상황에서 프롬프트의 중요성이 점점 낮아진다고 생각해왔다. 그러나 이번 경험이 그 생각을 재고하게 만들었다. 특히 저전력 저성능 디바이스에 탑재되는 경량 모델이 늘어날 미래를 고려하면, 프롬프트 엔지니어링의 가치는 오히려 다시 올라갈 수도 있겠다는 생각이 들었다. 

지금까지처럼 말로만 AI가 아닌 진짜 AI 세탁기, AI 냉장고, AI 청소기 등이 경량 LLM과 함께 보편화될 것이다.

## Gemini 무료 티어 중단

Google AI Studio의 무료 티어는 한때 개발자들에게 엄청 관대한 편이었다.  [[Spring Boot] Spring AI 활용해 LLM과 연동하기](https://shanepark.tistory.com/538) 글에서 언급했던 것처럼 RPM 30, RPD 1500 이라는 꽤나 넉넉한 밴드를 제공했다.

![4](https://raw.githubusercontent.com/ShanePark/mdblog/main/LLM/gemma3.assets/4.webp)

> 과거의 넉넉했던 rate limits

그러나 2025년 12월 7일을 기점으로 상황이 달라졌다. `gemini-2.5-pro`는 무료 티어에서 사실상 제거되었고, `gemini-2.5-flash`의 일일 요청 수는 250 RPD에서 20 RPD로 92% 이상 삭감되었다. 예고 없이 진행된 변경이라 많은 개발자들이 갑작스러운 429 에러를 마주하고 나서야 상황을 파악했다.

현재 무료로 쓸 수 있는 옵션의 실제 사용량 제한은 다음과 같다. 사실상 테스트용으로도 못쓴다고 봐야한다.

| 모델                  | RPM  | TPM  | RPD    |
| --------------------- | ---- | ---- | ------ |
| gemini-2.5-flash | 5   | 250K | 20     |
| gemini-2.5-flash-lite | 10   | 250K | 20     |

## Gemma 3

그래서 여러 가지 대안을 찾아보고 비교해본 결과 Gemma 3 가 최종 후보가 되었다.

Gemma 3는 Google이 2025년 3월에 공개한 오픈웨이트 모델이다. Gemini와 동일한 연구 기술을 기반으로 만들어졌지만, 오픈 웨이트로 공개되어 누구나 자유롭게 사용할 수 있다. 27B 버전의 주요 스펙은 다음과 같다.

- **파라미터**: 27B
- **컨텍스트 윈도우**: 128K
- **멀티모달**: 텍스트와 이미지 입력 지원
- **다국어 지원**: 140개 이상 언어

128K 컨텍스트 윈도우는 긴 문서나 대량의 데이터를 한 번에 처리할 수 있어 실용적이다. 다만 Gemini 계열과 비교하면 추론 능력에서 확연한 차이가 느껴진다. 

| 모델             | RPM  | TPM  | RPD  |
| ---------------- | ---- | ---- | ---- |
| gemma-3-12b | 30    | 15K | 14.4K   |
| gemma-3-27b | 30    | 15K | 14.4K   |

그래도 무료 사용량이 상당히 넉넉하고 기존의 gemini 사용 코드에서 모델명만 `gemma-3-27b-it` 로 바꾸면 별다른 설정이나 코드 변경 없이 바로 사용할 수 있다는 큰 장점이 있어 선택했다.

![2](https://raw.githubusercontent.com/ShanePark/mdblog/main/LLM/gemma3.assets/2.webp)

> 테스트 환경에서 limit 에 닿을 정도로 요청을 보내보니 어느정도 괜찮았다.

## 추론 능력의 한계와 프롬프트 엔지니어링

그러나 Gemma 3로 전환하고 기존 프롬프트 테스트 코드를 돌려보니 바로 실패했다. 다양한 조건의 텍스트를 정규화할 때 의도한 결과가 나오지 않는 케이스가 속출했다. Gemini에서는 별다른 설명 없이도 맥락을 파악하던 것들이 Gemma 3에서는 통하지 않았다.

결국 프롬프트를 대폭 수정해야 했다. 핵심적인 변화는 두 가지였다.

첫째, 프롬프트가 훨씬 상세하고 길어져야 했다. "이런 건 당연히 이렇게 처리하겠지"라는 암묵적 기대가 통하지 않았다. 모든 조건과 예외 상황을 명시적으로 기술해야 했다.

둘째, Few-shot prompting이 다시 필요해졌다. GPT-3~4 시절에 쓰던 기법이다. 입력과 출력의 예시를 여러 개 제공해서 패턴을 학습시키는 방식인데, 최신 모델에서는 잘 쓰지 않게 된 기법이 다시 효과를 발휘했다.

## Claude Code로 프롬프트 자동 최적화

프롬프트를 일일이 수정하고 테스트하는 과정이 번거로워서 Claude Code에게 맡겨보았다. 테스트 코드를 통과할 때까지 프롬프트를 스스로 수정하고 실행하는 피드백 루프를 구성했다. 한 가지 주의할 점은 테스트 코드에 과적합된 프롬프트를 만들지 않도록 가이드라인을 줘야 한다는 것이다. 실제로 초반에는 특정 테스트 케이스에만 맞춘 프롬프트를 생성하길래 그러지 말라고 지시해야 했다.

한참 동안 스스로 반복하더니 결국 모든 테스트를 통과하는 프롬프트가 완성되었다. 원래 프롬프트보다야 조금 길어졌지만, 실제 서비스에 반영하니 기존과 동일하게 잘 동작하는 것이 확인되었다.

## 마치며

Gemma 3의 추론 능력은 Gemini에 비해 확실히 떨어진다. 하지만 프롬프트 엔지니어링을 통해 실용적인 수준까지 끌어올릴 수 있었다. 프론티어 모델의 성능이 올라갈수록 프롬프트의 중요성은 낮아지겠지만, 경량 모델을 다뤄야 하는 상황은 앞으로도 계속 있을 것이다. Few-shot prompting 같은 고전적인 기법들이 여전히 유효하다는 점을 기억해둘 필요가 있다.

**References**

- https://github.com/cheahjs/free-llm-api-resources
- https://huggingface.co/google/gemma-3-27b-it